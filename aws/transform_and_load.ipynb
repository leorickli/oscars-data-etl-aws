{
 "metadata": {
  "kernelspec": {
   "name": "glue_pyspark",
   "display_name": "Glue PySpark",
   "language": "python"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "%connections redshift_database",
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nConnections to be included:\nredshift_database\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import re\nimport pandas as pd\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession",
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Initialize Spark and Glue context\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n# Load data from S3 using Glue catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(database=\"oscars-data\", table_name=\"bronze\")\n\n# Convert Glue DynamicFrame to Spark DataFrame\nspark_df = datasource.toDF()\n\n# Convert Spark DataFrame to Pandas DataFrame for processing\ndf = spark_df.toPandas()\n\n# Define the clean_budget function\ndef clean_budget(budget):\n    if isinstance(budget, (int, float)):\n        return str(budget)\n    budget = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', budget).strip()\n    if '–' in budget or '-' in budget:\n        budget = '0'\n    if budget.startswith('$'):\n        budget = budget.replace('$', 'US$', 1)\n    budget = budget.replace('USD$', 'US$')\n    budget = re.sub(r'(\\d)\\.(\\d{3})\\.(\\d{3})', r'\\1,\\2,\\3', budget)\n    match = re.match(r'(US\\$|£|€)\\s*([\\d\\.]+)\\s*million', budget, re.IGNORECASE)\n    if match:\n        currency = match.group(1)\n        amount = float(match.group(2))\n        budget = f'{currency} {amount * 1_000_000:,.0f}'\n    budget = budget.replace(' ', '')\n    return budget\n\n# Apply the clean_budget function to the budget column\ndf['budget'] = df['budget'].apply(clean_budget)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Static exchange rates\nexchange_rates = {\n    'GBP': 1.38,  # Example rate for GBP to USD\n    'EUR': 1.18   # Example rate for EUR to USD\n}\n\n# Function to convert currencies to USD using static rates\ndef convert_to_usd(budget):\n    budget = str(budget)\n    if budget == '0':\n        return 0\n    match = re.match(r'(US\\$|£|₤|€)\\s*([\\d,\\.]+)', budget)\n    if match:\n        currency = match.group(1)\n        amount = float(match.group(2).replace(',', ''))\n        if currency in ['£', '₤']:\n            amount_in_usd = amount * exchange_rates['GBP']\n        elif currency == '€':\n            amount_in_usd = amount * exchange_rates['EUR']\n        else:\n            amount_in_usd = amount\n        return int(amount_in_usd)\n    return 0\n\n# Apply the conversion function to create a new column\ndf['budget_in_usd'] = df['budget'].apply(convert_to_usd)\n\n# Ensure correct data types\ndf['film'] = df['film'].astype(str)\ndf['wiki_url'] = df['wiki_url'].astype(str)\ndf['budget'] = df['budget'].astype(str)\ndf['year'] = pd.to_numeric(df['year'], errors='coerce').fillna(0).astype(int)\ndf['oscar_winner'] = df['oscar_winner'].astype(str)\ndf['budget_in_usd'] = pd.to_numeric(df['budget_in_usd'], errors='coerce').fillna(0).astype(int)\n\n# Reorder columns to match the desired schema\ndf = df[['film', 'year', 'wiki_url', 'oscar_winner', 'budget', 'budget_in_usd']]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Convert Pandas DataFrame back to Spark DataFrame\nspark_df = spark.createDataFrame(df)\n\n# Convert Spark DataFrame to Glue DynamicFrame\ntransformed_dynamic_frame = DynamicFrame.fromDF(spark_df, glueContext, \"transformed_dynamic_frame\")\n\n# Define connection options for Redshift\nmy_conn_options = {\n    \"dbtable\": \"dev_oscars_data\",\n    \"database\": \"dev\"\n}\n\n# Write the DynamicFrame to Redshift\ntry:\n    redshift_results = glueContext.write_dynamic_frame.from_jdbc_conf(\n        frame = transformed_dynamic_frame,\n        catalog_connection = \"redshift_database\",\n        connection_options = my_conn_options,\n        redshift_tmp_dir = \"s3://yipitdata-bucket/redshift_temp/\"\n    )\n    print(\"Data written to Redshift successfully\")\nexcept Exception as e:\n    print(f\"Error writing data to Redshift: {e}\")",
   "metadata": {
    "trusted": true,
    "tags": []
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Data written to Redshift successfully\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for column, series in pdf.iteritems():\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for column, series in pdf.iteritems():\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
